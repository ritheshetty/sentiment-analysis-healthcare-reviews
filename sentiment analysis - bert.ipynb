{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ca775e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rithesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rithesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, BertTokenizer, TFBertForSequenceClassification, BertConfig, TFDistilBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "905725ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_transform', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = 3\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels, problem_type=\"multi_label_classification\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f107cb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  2307      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,955,779\n",
      "Trainable params: 66,955,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "044cb0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('BalancedReviews3Class.csv')\n",
    "dataset = dataset.iloc[:,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1a659ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If you have an emergency and drive yourself, b...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>See my tip.  While I really like my doctor and...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>very knowledgeable doctor and one who will fix...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I had question about my surgery. Called them f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My mother went to see Dr. Mcgee (podiatry) at ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12054</th>\n",
       "      <td>As a dentist, I was very please with Dr. Oakes...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12055</th>\n",
       "      <td>I've come to this office for three different i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12056</th>\n",
       "      <td>Absolutely horrible experience with this offic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12057</th>\n",
       "      <td>We stopped doing IV therapy because it's very ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12058</th>\n",
       "      <td>My initial impression of this LensCrafters was...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12059 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews  sentiment\n",
       "0      If you have an emergency and drive yourself, b...          2\n",
       "1      See my tip.  While I really like my doctor and...          2\n",
       "2      very knowledgeable doctor and one who will fix...          1\n",
       "3      I had question about my surgery. Called them f...          0\n",
       "4      My mother went to see Dr. Mcgee (podiatry) at ...          2\n",
       "...                                                  ...        ...\n",
       "12054  As a dentist, I was very please with Dr. Oakes...          2\n",
       "12055  I've come to this office for three different i...          0\n",
       "12056  Absolutely horrible experience with this offic...          0\n",
       "12057  We stopped doing IV therapy because it's very ...          1\n",
       "12058  My initial impression of this LensCrafters was...          0\n",
       "\n",
       "[12059 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70f9faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_punctuation(df):\n",
    "    df[\"reviews\"] = df[\"reviews\"].str.replace(\"[^a-zA-Z0-9#']\", \" \") \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dffd1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_words(df):\n",
    "    df['reviews'] = df['reviews'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e64500b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lowercase(df):\n",
    "    df['reviews'] = [review.lower() for review in df['reviews']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99148726",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(rev):\n",
    "    add_words = ['doctor']\n",
    "    stop_words.extend(add_words)\n",
    "    review_tokenized = word_tokenize(rev)\n",
    "    rev_new = \" \".join([i for i in review_tokenized  if i not in stop_words])\n",
    "    return rev_new\n",
    "    \n",
    "def stopwords_processing(df):\n",
    "    df['reviews'] = [remove_stopwords(r) for r in df['reviews']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8009223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_sentence(sentence):\n",
    "    nltk_tagged = nltk.pos_tag(word_tokenize(sentence))  \n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "def lemmatize(df):\n",
    "    df['reviews'] = df['reviews'].apply(lambda x: lemmatize_sentence(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20099979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = replace_punctuation(df)\n",
    "    df = remove_short_words(df)\n",
    "    df = make_lowercase(df)\n",
    "    df = stopwords_processing(df)\n",
    "    df = lemmatize(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c35852e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rithesh\\AppData\\Local\\Temp\\ipykernel_9744\\1816754521.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"reviews\"] = df[\"reviews\"].str.replace(\"[^a-zA-Z0-9#']\", \" \")\n"
     ]
    }
   ],
   "source": [
    "dataset = preprocess(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0b80366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(dataset, test_size = 0.3, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e347c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>leave first appointment saw elizabeth bleeker ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>note covid pandemic refer endoscopy n't teledo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>wait time ridiculous good doctor front dess sc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5309</th>\n",
       "      <td>agree reviewer stay away place go 2010 routine...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824</th>\n",
       "      <td>try fit many eye exam one day always run behin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews  sentiment\n",
       "2031   leave first appointment saw elizabeth bleeker ...          1\n",
       "10012  note covid pandemic refer endoscopy n't teledo...          2\n",
       "4195   wait time ridiculous good doctor front dess sc...          0\n",
       "5309   agree reviewer stay away place go 2010 routine...          0\n",
       "7824   try fit many eye exam one day always run behin...          0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c0a94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
    "test.columns = ['DATA_COLUMN', 'LABEL_COLUMN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a68a379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN):\n",
    "    train_examples = train.apply(lambda x : InputExample(guid = None,\n",
    "                                                        text_a = x[DATA_COLUMN],\n",
    "                                                        text_b = None,\n",
    "                                                        label = x[LABEL_COLUMN]), axis = 1)\n",
    "    validation_examples = test.apply(lambda x : InputExample(guid = None,\n",
    "                                                        text_a = x[DATA_COLUMN],\n",
    "                                                        text_b = None,\n",
    "                                                        label = x[LABEL_COLUMN]), axis = 1)\n",
    "    return train_examples, validation_examples\n",
    "\n",
    "DATA_COLUMN = 'DATA_COLUMN'\n",
    "LABEL_COLUMN = 'LABEL_COLUMN'\n",
    "\n",
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train, \n",
    "                                                                        test, \n",
    "                                                                        DATA_COLUMN, \n",
    "                                                                        LABEL_COLUMN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feadecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTILBERT\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] \n",
    "    for e in examples:\n",
    "       \n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, \n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            padding='max_length', \n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "        \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "#                     \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "#                 \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "type(train_data)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd549027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BERT\n",
    "# def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "#     features = [] \n",
    "#     for e in examples:\n",
    "#         # Documentation is really strong for this method, so please take a look at it\n",
    "#         input_dict = tokenizer.encode_plus(\n",
    "#             e.text_a,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=max_length, # truncates if len(s) > max_length\n",
    "#             return_token_type_ids=True,\n",
    "#             return_attention_mask=True,\n",
    "#             padding='max_length', # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "#             truncation=True\n",
    "#         )\n",
    "\n",
    "#         input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "#             input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "        \n",
    "#         features.append(\n",
    "#             InputFeatures(\n",
    "#                 input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     def gen():\n",
    "#         for f in features:\n",
    "#             yield (\n",
    "#                 {\n",
    "#                     \"input_ids\": f.input_ids,\n",
    "#                     \"attention_mask\": f.attention_mask,\n",
    "#                     \"token_type_ids\": f.token_type_ids,\n",
    "#                 },\n",
    "#                 f.label,\n",
    "#             )\n",
    "\n",
    "#     return tf.data.Dataset.from_generator(\n",
    "#         gen,\n",
    "#         ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "#         (\n",
    "#             {\n",
    "#                 \"input_ids\": tf.TensorShape([None]),\n",
    "#                 \"attention_mask\": tf.TensorShape([None]),\n",
    "#                 \"token_type_ids\": tf.TensorShape([None]),\n",
    "#             },\n",
    "#             tf.TensorShape([]),\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "# train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "# train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "# print(type(train_data))\n",
    "\n",
    "# validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "# validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "884bb82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40392724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:From C:\\Users\\Rithesh\\anaconda3\\envs\\ks295p\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "528/528 [==============================] - 6266s 12s/step - loss: 0.1765 - accuracy: 0.9405 - val_loss: 0.2455 - val_accuracy: 0.9237\n",
      "Epoch 2/2\n",
      "528/528 [==============================] - 6065s 11s/step - loss: 0.0718 - accuracy: 0.9776 - val_loss: 0.2570 - val_accuracy: 0.9290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2138cf818b0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(train_data, epochs=2, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1caf6e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d2043ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a98333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rithesh\\anaconda3\\envs\\ks295p\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FC5BD2E6D0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FC5BF6FCA0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FC5BF86C40>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FC5BF95BE0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FC5BFA1B80>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FC5BFB0B20>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, transformer_layer_call_fn, transformer_layer_call_and_return_conditional_losses while saving (showing 5 of 165). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DistilBert\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DistilBert\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('DistilBert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db2aa8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = ['Very good doctor', \n",
    "                  'the doctor was as good as my 6 year old',\n",
    "                  'the clinic was clean but the doctor could be better tbh', \n",
    "                  'can safely say that this is an bad place',\n",
    "                 'is this doctor even for real? I cannot believe this service, so good',\n",
    "                 'poorest service']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c361c3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(6, 3), dtype=float32, numpy=\n",
      "array([[-3.621418  ,  2.4831648 ,  0.74470097],\n",
      "       [-2.86753   ,  0.46841314,  2.1549695 ],\n",
      "       [-3.2287614 ,  1.4268769 ,  1.4378314 ],\n",
      "       [ 4.0951295 , -3.0556383 , -1.0528983 ],\n",
      "       [-1.3185476 , -0.47535902,  1.4856945 ],\n",
      "       [ 3.7923725 , -3.4539533 , -0.4716779 ]], dtype=float32)>, hidden_states=None, attentions=None)\n",
      "0.8391063213348389 \n",
      "\n",
      "Very good doctor : \n",
      " Positive\n",
      "the doctor was as good as my 6 year old : \n",
      " Neutral\n",
      "the clinic was clean but the doctor could be better tbh : \n",
      " Neutral\n",
      "can safely say that this is an bad place : \n",
      " Negative\n",
      "is this doctor even for real? I cannot believe this service, so good : \n",
      " Neutral\n",
      "poorest service : \n",
      " Negative\n"
     ]
    }
   ],
   "source": [
    "tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "tf_outputs = model(tf_batch)\n",
    "print(tf_outputs)\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "print(float(tf_predictions[1][2]),\"\\n\")\n",
    "labels = ['Negative','Positive','Neutral']\n",
    "label = tf.argmax(tf_predictions, axis=1)\n",
    "label = label.numpy()\n",
    "for i in range(len(pred_sentences)):\n",
    "    print(pred_sentences[i], \": \\n\", labels[label[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1a9f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment score computation function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
